---
layout: post
title: From Big-O to lim sup
---

## big-O
The definition is *little-o* is often described in terms of taking a limit and checking is it is equal to zero. On the other hand, the definition of *big-O* is usually expressed in terms of the existence or certain positive constants satisfying an inequality. The limit definition for *little-o* is very useful and it would be nice if we had a similar definition for *big-O*. Let's see what the issues are here. Let's start with the definition of *big-O*,

$$f(n)$$ is $$O(g(n))$$ if there exist positive constants $$c$$ and $$N$$, such that for all $$n \geq N$$, \\[f(n) \leq cg(n)\\] Contrast this with the definition of *little-o*,

$$f(n)$$ is $$O(g(n))$$ if for all $$c > 0$$, there exist $$N$$, such that for all $$n \geq N$$, \\[f(n) \leq cg(n)\\]

*Note that in the definition of *little-o*, $$N$$ may depend on the choice of $$c$$.*
An equivalent definition of *little-o* is,

$$f(n)$$ is $$o(g(n))$$ if
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 0
$$
{% endraw %}

This is the definition most often used for checking if $$f(n)$$ is $$o(g(n))$$, since we can apply the tools of calculus to calculate the limit instead of trying to find an $$N$$ for each $$c$$. Do we have a similar definition of *big-O*? It may seem at first that the following definition is equivalent to the above definition for *big-O*,

$$f(n)$$ is $$O(g(n))$$ if,
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty
$$
{% endraw %}

The problem is that the limit is not guaranteed to exist. This was not a problem in our definition of *little-o*, since if the limit is equal $$0$$ it must exit. Here is an example of where the limit does not exit. Take $$f(n)$$ to be the function on the integers $$\{0,1, \ldots\}$$ such that $$f(n)=0$$ if $$n$$ is even and $$f(n)=1$$ if $$n$$ is odd, and let $$g(n)=1$$. Then the limit as $$n \rightarrow \infty$$ of $$f(n)/g(n)$$ does not exist, the value of $$f(n)/g(n)$$ oscillates between $$0$$ and $$1$$ forever. On the other hand $$f(n)$$ is clearly $$O(g(n))$$, since
{% raw %}
$$f(n) \leq g(n) \text{ for all } n \geq 0$$
{% endraw %}

This tells us that if the definition of *big-O* is satisfied it may not be the case that the above limit exists. What about the other direction, i.e. if the limit exists and is finite, does this imply *big-O*, after all, we are usually trying to prove that some function is *big-O* of some other function. Let's see, if the limit is finite it must be equal to some constant $$c$$ so we have,
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} \leq c \\
\text{This means that there must be a $c$ and an $N$ such that, for all $n \geq N$} \\
\frac{f(n)}{g(n)} \leq c \\
\text{that is} \\
f(n) \leq cg(n)
$$
{% endraw %}
Voila, that's the definition of *big-O*, so in fact if the limit exits and is finite then *big-O* is implied. That's good, it means that if we can calculate the limit, then we can show *big-O* if the limit is positive (It turns out that this implies not only *big-O*, but *big-*$$\theta$$ as well) and also *little-o* if it turns out to be $$0$$. Still it would be nice to have a definition that worked even when the limit doesn't exist like in our oscillating function from before.

## limsup to the rescue

OK, so just what is the $$\limsup$$ otherwise known as the limit superior. Here is the formal definition,
{% raw %}
$$\limsup_{n \rightarrow \infty} h(n) = \lim_{n \rightarrow \infty} \sup_{j \geq n} h(j)$$
{% endraw %}
Let's take this piece by piece. First, what does $$\sup$$ mean in $$sup_{j \geq n} h(j)$$. Easy, $$\sup$$ means least upper bound. $$\sup$$ is very similar to $$\max$$, so $$\sup_{j \geq n} h(j)$$ is like the maximum value of $$h(j)$$ on the set of all integers $$j \geq n$$. The only difference between $$\max$$ and $$\sup$$ is that $$h$$ must attain it's maximum, i.e. there must be some $$k \geq n$$ such that $$\max_{j \geq n} h(j) = h(k)$$. This means that the $$\max$$ may not always exist. But the $$\sup$$ is guaranteed to exist. For example suppose $$h(n)$$ is the function $$1-2^{-n}$$, that has a horizontal asymptote and $$y=1$$. This function does not have a $$max$$, if you give me any number less than $$1$$ I'll give you an $$n$$ making $$h(n)$$ bigger. On the other hand $$h(n)$$ is always less than $$1$$ and there is no smaller number for which this is true, so $$1$$ is the $$\sup_{n \geq 0} h(n)$$. Nice, using $$sup$$ guarantees we don't get into trouble with cases where the maximum is not attained.

Now that we understand what $$sup$$ is lets, replace the limit with it's definition and see what it means for the $$\limsup$$ to be finite. That is,
{% raw %}
$$\lim_{n \rightarrow \infty} \sup_{j \geq n} h(j) < \infty$$
{% endraw %}
is equivalent to the statement that there exists a positve constant $$N$$ such that for all $$n \geq N$$,
{% raw %}
$$\sup_{j \geq n} h(j) < \infty$$
{% endraw %}
