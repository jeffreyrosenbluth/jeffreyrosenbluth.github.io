---
layout: post
title: From Big-O to lim sup
---

## big-O
The definition of *little-o* is often described in terms of taking a limit and checking that it is equal to zero. On the other hand, the definition of *big-O* is usually expressed in terms of the existence of certain positive constants satisfying an inequality. The "limit" definition for *little-o* is very useful and it would be nice if we had a similar definition for *big-O*. Let's see what the issues are. We start with a formal definition of *big-O*. We say that,

$$f(n)$$ is $$O(g(n))$$ if there exist positive constants $$c$$ and $$N$$, such that for all $$n \geq N$$, \\[f(n) \leq cg(n)\\] By the way, I'm aiming this post at computer science students and thus I'll will assume throughout that the functions we analyze are non-negative)\. Contrast this with the definition of *little-o*,

$$f(n)$$ is $$o(g(n))$$ if for all $$c > 0$$, there exist $$N$$, such that for all $$n \geq N$$, \\[f(n) \leq cg(n)\\]

*Note that in the definition of little-o*, $$N$$ may depend on the choice of $$c$$. The difference is that this inequality must hold for **all** $$c$$, so we can pick $$c$$ to be arbitrarily small. An equivalent definition of *little-o* is,

$$f(n)$$ is $$o(g(n))$$ if
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 0
$$
{% endraw %}

This is the definition most often used for checking if $$f(n)$$ is $$o(g(n))$$, since we can apply the tools of calculus, including e.g. l'hospital's rule, to calculate the limit instead of trying to find an $$N$$ for each $$c$$, which is usually more difficult. Do we have a similar definition for *big-O*? It may seem at first that the following is an equivalent to the one above, i.e. that a necessary and sufficient condition for $$f(n)$$ to be $$O(g(n))$$ is,
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty
$$
{% endraw %}

Unfortunately, this is **not** a valid definition of *big-O*. The problem is that the limit is not guaranteed to exist. This was not a problem in our definition of *little-o*, since if the limit is equal $$0$$ it must exit.

Here is an example of the trouble we could run into. Take $$f(n)$$ to be the function on the integers $$\{0,1, \ldots, \}$$ such that
{% raw %}
$$
f(n) =
\begin{cases}
    0 & \text{if $n$ is even,}\\
    1 & \text{if $n$ is odd}
\end{cases}
$$
{% endraw %}
and let $$g(n)=1$$. Then the limit as $$n \rightarrow \infty$$ of $$f(n)/g(n)$$ does not exist, it's value oscillates between $$0$$ and $$1$$ forever. On the other hand $$f(n)$$ is clearly $$O(g(n))$$, since
{% raw %}
$$f(n) \leq g(n) \textrm{ for all } n \geq 0$$
{% endraw %}

This tells us that if the definition of *big-O* is satisfied it may not be the case that the above limit exists. What about the other direction, i.e. if the limit exists and is finite, does this imply *big-O*, after all, we are usually trying to prove that some function is *big-O* of some other function. Let's see, if the limit is finite it must be equal to some constant $$c$$ so we have,
{% raw %}
$$
\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} \leq c \\
\text{This means that there must be a $c$ and an $N$ such that, for all $n \geq N$} \\
\frac{f(n)}{g(n)} \leq c \\
\text{that is} \\
f(n) \leq cg(n)
$$
{% endraw %}
Voila, that's the definition of *big-O*, so in fact if the limit exits and is finite then *big-O* is implied. That's good, it means that if we can calculate the limit, then we can show *big-O* if the limit is positive (It turns out that this implies not only *big-O*, but *big-*$$\theta$$ as well) and also *little-o* if it turns out to be $$0$$. Still it would be nice to have a definition that worked even when the limit doesn't exist like in our oscillating function from before.

## limsup to the rescue

OK, so just what is the $$\limsup$$ otherwise known as the limit superior. Here is the formal definition,
{% raw %}
$$\limsup_{n \rightarrow \infty} h(n) = \lim_{n \rightarrow \infty} \sup_{j \geq n} h(j)$$
{% endraw %}
Let's take this piece by piece. First, what does $$\sup$$ mean in $$sup_{j \geq n} h(j)$$. Easy, $$\sup$$ means least upper bound. $$\sup$$ is very similar to $$\max$$, so $$\sup_{j \geq n} h(j)$$ is like the maximum value of $$h(j)$$ on the set of all integers $$j \geq n$$. The only difference between $$\max$$ and $$\sup$$ is that $$h$$ must attain it's maximum, i.e. there must be some $$k \geq n$$ such that $$\max_{j \geq n} h(j) = h(k)$$. This means that the $$\max$$ may not always exist. But the $$\sup$$ is guaranteed to exist. For example suppose $$h(n)$$ is the function $$1-2^{-n}$$, that has a horizontal asymptote at $$y=1$$. This function does not have a $$max$$, if you give me any number less than $$1$$ I'll give you an $$n$$ making $$h(n)$$ bigger. On the other hand $$h(n)$$ is always less than $$1$$ and there is no smaller number for which this is true, so $$1$$ is the $$\sup_{n \geq 0} h(n)$$. Nice, using $$sup$$ guarantees we don't get into trouble with cases where the maximum is not attained.

Now that we understand what $$sup$$ is lets, replace the limit with it's definition and see what it means for the $$\limsup$$ to be finite. If something is finite it must be less than some constant $$c$$. That is,
{% raw %}
$$\lim_{n \rightarrow \infty} \sup_{j \geq n} h(j) \leq c < \infty$$
{% endraw %}
is equivalent to the statement that there exists a positive constant $$N$$ such that for all $$n \geq N$$,
{% raw %}
$$\sup_{j \geq n} h(j) \leq c$$
{% endraw %}
observe that $$sup_{j \geq n} h(j) \leq c$$ is the same as saying that all values of $$h(j)$$ on the set $$\{j \geq n\}$$ are less than $$c$$, since if the $$sup$$ is less than $$c$$ all values are. Finally, we arrive at the statement, there exist positive constants $$N$$ and $$c$$ such that for all $$n \geq N$$,
{% raw %}
$$h(n) \leq c$$
{% endraw %}
All we have to do now is replace $$h(n)$$ with $$f(n)/g(n)$$ to get
{% raw %}
$$
\frac{f(n)}{g(n)} \leq c \\
\text{or} \\
f(n) \leq cg(n)
$$
{% endraw %}
and we have arrived at the definition of *big-O*, proving that a finite $$\limsup$$ implies *big-O*, proving that *big-O* implies that the $$\limsup$$ is finite is pretty much the same proof in reverse. Hence we have shown that the two definitions are equivalent! That is, an alternative definition of $$f(n)$$ is $$O(g(n))$$ is
{% raw %}
$$\limsup_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty$$
{% endraw %}
The only difference between this result and our naive guess is that we had to use $$\limsup$$ instead of $$\lim$$ to handle cases where the limit does not exist.
